---
title: "Stable softmax with temperature"
date: "2024-01-05"
summary: "Implement a numerically stable softmax and verify gradient behavior."
tags: ["softmax", "numerical-stability"]
---

# Problem

Implement a temperature-scaled softmax that avoids overflow for large logits.

```python
import numpy as np

def softmax(logits, temperature=1.0):
    logits = np.asarray(logits) / temperature
    logits = logits - np.max(logits, axis=-1, keepdims=True)
    exp_logits = np.exp(logits)
    return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)
```

# Notes

- Subtract the max per row before exponentiating.
- Temperature > 1 flattens, < 1 sharpens.
